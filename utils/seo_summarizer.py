# -*- coding: utf-8 -*-
"""SEO Summarizer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dW579ggmLyLDnAg3NrWPBSJpgKPDmgQq
"""

# !pip install langchain-groq langchain langchain-community langchain-huggingface langgraph playwright chromadb

from google.colab import userdata
from langchain_groq import ChatGroq
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.schema import Document  
from langchain_core.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser


groq_api_key = userdata.get('GROQ_API_KEY')


llm = ChatGroq(model='llama-3.3-70b-versatile', api_key=groq_api_key)


embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
vector_store = InMemoryVectorStore(embeddings)




# Example HTML content as a string
html_string = """
<html>
    <head><title>Test Page</title></head>
    <body><h1>Hello, World!</h1></body>
</html>
"""

# Manually create a Document object to match WebBaseLoader output format
html = [Document(page_content=html_string)]



prompt = PromptTemplate.from_template("""You will be provided with the contents of a webpage.
Your task is to analyze the contents, summarize its contents in no more than 200 words, and optimize the summary for SEO.
<context>
{context}
</context>
Question={input}
Follow these steps:
    1. Summarize the primary content of the website accurately.
    2. Optimize the summary for SEO, incorporating relevant keywords and phrases naturally.
    3. Ensure the summary is concise, clear, and engaging for SEO bots.
    4. Generate ONLY the summary, no need for any additional statements
    5. The summary should ONLY be in a simple HTML format with proper structure fit for SEO
    6. The output html SHOULD have head section body section and proper meta tags
""")



text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
docs = text_splitter.split_documents(html)
_ = vector_store.add_documents(documents=docs)
retriever = vector_store.as_retriever()



rag_chain = (
        {"context": retriever, "input": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

summary = rag_chain.invoke("Summarize")

print(summary)
